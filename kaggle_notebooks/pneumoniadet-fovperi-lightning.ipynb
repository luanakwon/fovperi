{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10338,"databundleVersionId":862042,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of data distribution\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport pydicom\n\ntrain_labels = pd.read_csv('/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_labels.csv')\n\n#-------------------------------\nprint('Target distribution')\nn_zeros = len(train_labels[train_labels['Target']==0])\nn_ones = len(train_labels[train_labels['Target']==1])\nn_else = len(train_labels)-n_zeros-n_ones\nprint(f\"Ones: {n_ones}, Zeros: {n_zeros}, else: {n_else}\")\n\n#------------------------------\nprint('bounding box position distribution')\nbboxes = train_labels[['x','y','width','height']].dropna()\nx_info = {\n    'min': np.min(bboxes['x']),\n    'mean': np.mean(bboxes['x']),\n    'max': np.max(bboxes['x']),\n    'std': np.std(bboxes['x'])\n}\ny_info = {\n    'min': np.min(bboxes['y']),\n    'mean': np.mean(bboxes['y']),\n    'max': np.max(bboxes['y']),\n    'std': np.std(bboxes['y'])\n}\nw_info = {\n    'min': np.min(bboxes['width']),\n    'mean': np.mean(bboxes['width']),\n    'max': np.max(bboxes['width']),\n    'std': np.std(bboxes['width'])\n}\nh_info = {\n    'min': np.min(bboxes['height']),\n    'mean': np.mean(bboxes['height']),\n    'max': np.max(bboxes['height']),\n    'std': np.std(bboxes['height'])\n}\nprint(f\"bbox x distribution - {x_info}\")\nprint(f\"bbox y distribution - {y_info}\")\nprint(f\"bbox w distribution - {w_info}\")\nprint(f\"bbox h distribution - {h_info}\")\n\n#-----------------------------\nprint('pixel data distribution')\npatient100 = train_labels['patientId'].sample(100)\np_info = {'min':1e+10,'mean':0,'max':0,'std':0}\nfor pid in patient100:\n    dcm_root_path = '/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_train_images'\n    dcm_path = os.path.join(dcm_root_path,f'{pid}.dcm')\n    img = pydicom.read_file(dcm_path).pixel_array\n    p_info['min'] = min(p_info['min'],np.min(img))\n    p_info['mean'] += np.mean(img)\n    p_info['max'] = max(p_info['max'],np.max(img))\n    p_info['std'] += np.std(img)\n    \np_info['mean'] /= 100\np_info['std'] /= 100\n\nprint(f\"pixel value distribution - {p_info}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-24T07:11:37.299151Z","iopub.execute_input":"2024-05-24T07:11:37.299576Z","iopub.status.idle":"2024-05-24T07:11:39.564771Z","shell.execute_reply.started":"2024-05-24T07:11:37.299546Z","shell.execute_reply":"2024-05-24T07:11:39.563469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightning as L\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as F\n\n\nclass CustomTransform(nn.Module):\n    # totensor\n    # random horizontal flip\n    # random crop\n    # normalize\n    def __init__(self, size, random=True):\n        self.size = size\n        self.p = 0.5\n        self.mean = 126\n        self.std = 57\n        self.random=random\n        \n    def forward(self, data):\n        # split image and bbox label\n        img, bbox = data\n        width = img.shape[-1]\n        # numpy to tensor\n        img = torch.from_numpy(img, dtype=torch.float32)\n        if self.random:\n            # random horizontal flip\n            if torch.rand(1) < self.p:\n                img = F.hflip(img)\n                bbox[0] = width-bbox[0]-bbox[2]\n            # random crop (did not implement padding)\n            i, j, h, w = transforms.RandomCrop.get_params(img, self.size)\n            img = F.crop(img,i,j,h,w)\n            bbox[0] = bbox[0] - j if bbox[0] > j else 0\n            bbox[1] = bbox[1] - i if bbox[1] > i else 0\n            bbox[2] = w-bbox[0]-1 if bbox[0]+bbox[2] >= w else bbox[2]\n            bbox[3] = h-bbox[1]-1 if bbox[1]+bbox[3] >= h else bbox[3]\n        # normalize\n        img = F.normalize(img, self.mean, self.std)\n        \n        return img, bbox\n    \n    def __repr__(self):\n        out = \"Custom Transform to transform both the image and the bbox\\n\"\n        out += \"\\ttorch.from_numpy()\\n\"\n        if self.random:\n            out += f\"\\tRandomHorizontalFlip(p={self.p})\\n\"\n            out += f\"\\tRandomCrop({self.size}, padding=False)\\n\"\n        out += f\"\\tNormalize(mean={self.mean},std={self.std})\"\n        return out\n        \n        \nclass CustomDataset(Dataset):\n    def __init__(self, root, df, transform):\n        super(MyDataset).__init__()\n        self.root = root\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        # dtype of float32 implemented only\n        # convert bbox to tensor\n        bbox = torch.Tensor(row[1:5], dtype=torch.float32)\n        label = torch.Tensor(row[5], dtype=torch.float32)\n        \n        pid = row[0]\n        dcm_path = os.path.join(self.root,f'{pid}.dcm')\n        img = pydicom.read_file(dcm_path).pixel_array\n        \n        img, bbox = self.transform((img,bbox))\n        \n        return pid, img, bbox, label\n        \n    \n\nclass PDCDataModule(L.LightningDataModule):\n    def __init__(self, data_dir='./', batch_size=1,num_workers=0):\n        super().__init__()\n        self.data_dir = data_dir\n        if isinstance(batch_size,int):\n            self.tr_batch_size=batch_size\n            self.val_batch_size=batch_size\n        elif len(batch_size) == 2:\n            self.tr_batch_size=batch_size[0]\n            self.val_batch_size=batch_size[1]\n        else:\n            raise ValueError(batch_size) \n        self.num_workers=num_workers\n        \n    def prepare_data(self, fold=0, random_seed=42)\n        # read full dataframe\n        full_df = pd.read_csv(os.path.join(\n            self.data_dir,'stage_2_train_labels.csv'))\n        df_0 = full_df[full_df['Target']==0]\n        df_1 = full_df[full_df['Target']==1]\n        # apply undersampling to target==0\n        df_00 = df_0.sample(frac=0.25,random_state=random_seed)\n        df_01 = df_0.drop(df_00.index).sample(n=len(df_00),random_state=random_seed)\n        df_10 = df_1.sample(frac=0.5,random_state=random_seed)\n        df_11 = df_1.drop(df_10.index)\n        # Train Test Split: split the dataframe\n        if fold == 0:\n            self.tr_df = pd.concat((df_00,df_10))\n            self.val_df = pd.concat((df_01,df_11))\n        elif fold == 1:\n            self.tr_df = pd.concat((df_01,df_11))\n            self.val_df = pd.concat((df_00,df_10))\n        else:\n            raise ValueError('fold should be either 0 or 1')\n        \n    def setup(self, size):\n        self.tr_dset = CustomDataset(\n            root=os.path.join(self.data_dir,'stage_2_train_images'),\n            df=self.tr_df,\n            transform=CustomTransform(size))\n        self.val_dset = CustomDataset(\n            root=os.path.join(self.data_dir,'stage_2_train_images'),\n            df=self.val_df,\n            transform=CustomTransform(size, random=False))\n    \n    def train_dataloader(self):\n        return DataLoader(\n                    dataset=self.tr_dset, \n                    batch_size=self.tr_batch_size, \n                    suffle=True, num_workers=self.num_workers)\n    def val_dataloader(self):\n        return DataLoader(\n                    dataset=self.val_dset, \n                    batch_size=self.val_batch_size, \n                    shuffle=False, num_workers=self.num_workers)\n    \n    \n# lightning module hooks guide.\n# https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#lightning-hooks\nclass LitMobileNetv2(L.LightningModule):\n    def __init__(self, nr, p0, pmax, n_saccade):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = torch.hub.load('pytorch/vision:v0.10.0','mobilenet_v2',pretrained=True)\n        self.model.features[0][0] = nn.Conv2d(\n            1,32,kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        self.model.classifier =  \\\n            torch.nn.Linear(in_features=1280, out_features=1, bias=True)\n        self.fovea_radius = p0\n        self.base_fov = get_inv_FCG_mapping(nr, p0, pmax, (0,0))\n        self.un_fov_func = get_FCG_revertFunc(nr,p0,pmax)\n        self.n_sacc = n_saccade\n        \n    def transfer_batch_to_device(self, batch, device, dataloader_idx):\n        # Override this to prevent batch transfer at this stage\n        # Since in the training step multiple forward is required\n        # batch to device is manually done in the training step\n        return batch\n    \n    def training_step(self, batch, batch_idx):\n        b_pid, b_img, b_bbox, b_label = batch\n        # transfer label to device. \n        # This can be done in the transfer_batch_to_device hook\n        b_label = b_label.to(self.device)\n        b, c, h, w = b_img.shape\n        # initialize zoom/scale to 1\n        scale = torch.ones((b,2))\n        # initialize saccade/translate to the center of the original image\n        translate = torch.ones((b,2))*torch.tensor([w//2,h//2])\n        # init loss. losses are acculumated for all of the saccades\n        total_loss = 0\n        for _ in range(self.n_sacc):\n            # get foveated batch \n            b_fov = batch_remap(b_img, self.base_fov, scale, translate, border_value=0)\n            # transfer only the foveated to the device\n            b_fov = b_fov.to(self.device)\n            pred, attn_info = self.model(b_fov)\n            # calculate loss\n            loss = F.binary_cross_entropy_with_logits(pred,b_label)\n            # accumulate loss\n            total_loss += loss\n            \n            # undo foveation to the attn_info(which is y,x,r)\n            attn_loc_0 = attn_info[:,:2] - torch.tensor(b_fov.shape[2:])\n            attn_loc = self.un_fov_func(attn_loc)\n            coef = ( torch.max(attn_loc,dim=1).values \n                    / torch.max(attn_loc_0,dim=1).values )\n            attn_r = coef*attn_info[:,2]\n            attn_loc = attn_loc*scale+translate\n            # update saccade (scale & translate)\n            scale = (attn_r/self.fovea_radius).view(-1,1).repeat_interleave(2,1)\n            translate = attn_loc\n            \n            # any logging if necessary\n        return total_loss / self.n_sacc\n        \n            \n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-24T10:52:35.988070Z","iopub.execute_input":"2024-05-24T10:52:35.988658Z","iopub.status.idle":"2024-05-24T10:52:36.049425Z","shell.execute_reply.started":"2024-05-24T10:52:35.988627Z","shell.execute_reply":"2024-05-24T10:52:36.047943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CustomMobileNetV2(nn.Module):\n    def __init__(self):\n        super(CustomMobileNetV2, self).__init__()\n        base = torch.hub.load('pytorch/vision:v0.10.0','mobilenet_v2',pretrained=True)\n        self.input_layer = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU6(inplace=True))\n        self.bottom_features = nn.Sequential(*base.features[1:14])\n        self.top_features = nn.Sequential(*base.features[14:])\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(in_features=1280, out_features=1, bias=True))\n        \n        # weight initialization\n        for m in self.input_layer.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                # omitted bias\n            elif isinstance(m, (nn.BatchNorm2d)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n        for m in self.classifier.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n        \n    @staticmethod\n    def get_attn_loc(attn):\n        b, h, w = attn.shape\n        max_v, max_i = torch.max(attn.flatten(start_dim=1),dim=1)\n        max_y = max_i // w\n        max_x = max_i % w\n        # calculate the size of the area around the max\n        # where 90% of the values are above average\n        bool_attn = attn>torch.mean(attn,dim=(1,2)).view(-1,1,1)\n        r_batch = torch.ones((b))\n        for bid, (b_at, y, x) in enumerate(zip(bool_attn,max_y,max_x)):\n            max_r = min((y,h-y,x,w-x))\n            for r in range(3,max_r):\n                p_above_avg = torch.sum(b_at[y-r:y+r+1,x-r:x+r+1]) / ((2*r+1)**2)\n                if p_above_avg < 0.9:\n                    r_batch[bid] = r-1\n        return torch.stack([max_y,max_x,r_batch]).T\n    \n    \n    def _forward_impl(self,x):\n        # This exists since TorchScript doesn't support inheritance, so the superclass method\n        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n        x = self.input_layer(x)\n        x = self.bottom_features(x)\n        # Insert attention block after 13 InvertedResidual block \n        # to keep 16:1 reduce in resolution in the attention map\n        # as they did in the paper, use the mean across the features\n        attn = torch.mean(x.detach(),dim=1)\n        locs = CustomMobileNetV2.get_attn_loc(attn)  \n        # continue forward \n        x = self.top_features(x)\n        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\n        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n        x = self.classifier(x)\n        return x, locs\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\n    \nprint(CustomMobileNetV2())","metadata":{"execution":{"iopub.status.busy":"2024-06-01T06:56:07.029485Z","iopub.execute_input":"2024-06-01T06:56:07.029940Z","iopub.status.idle":"2024-06-01T06:56:07.187208Z","shell.execute_reply.started":"2024-06-01T06:56:07.029907Z","shell.execute_reply":"2024-06-01T06:56:07.185688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PrinterNet(torch.nn.Module):\n    def __init__(self):\n        super(PrinterNet, self).__init__()\n        \n    def forward(self, x):\n        print(x.shape)\n        return x\n\nnew_features = []\nmodel = torch.hub.load('pytorch/vision:v0.10.0','mobilenet_v2',pretrained=True)\nfor i, feature in enumerate(model.features):\n    new_features.append(feature)\n    if i != 0:\n        new_features.append(PrinterNet())\n        \nmodel.features = torch.nn.Sequential(*new_features)\n# print(model.features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.features[0][0])\n\nmodel.features[0][0] = \\\n    torch.nn.Conv2d(\n        1,32,kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\nmodel.classifier[1] = \\\n    torch.nn.Linear(in_features=1280, out_features=1, bias=True)\n\nprint(model.features[0])\nprint(model.classifier)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T04:41:06.550678Z","iopub.execute_input":"2024-06-01T04:41:06.551058Z","iopub.status.idle":"2024-06-01T04:41:06.560358Z","shell.execute_reply.started":"2024-06-01T04:41:06.551028Z","shell.execute_reply":"2024-06-01T04:41:06.558882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomMobileNetV2()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T06:57:25.002959Z","iopub.execute_input":"2024-06-01T06:57:25.003374Z","iopub.status.idle":"2024-06-01T06:57:25.136819Z","shell.execute_reply.started":"2024-06-01T06:57:25.003337Z","shell.execute_reply":"2024-06-01T06:57:25.135659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\nimg = pydicom.read_file('/kaggle/input/rsna-pneumonia-detection-challenge/stage_2_test_images/0000a175-0e68-4ca4-b1af-167204a7e0bc.dcm')\nimg = img.pixel_array\n\nprint(img.shape, type(img))\nprint(np.min(img),np.mean(img), np.max(img))\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485], std=[0.229]),\n])\n\nx = preprocess(img[::4,::4])\nx = x.unsqueeze(0)\nprint(x.shape)\nprint(torch.min(x),torch.mean(x), torch.max(x))\n\nmodel.eval()\nwith torch.no_grad():\n    out, locs = model(x)\n    \nprint(out.shape, locs.shape)\nconf = torch.nn.functional.sigmoid(out[0])\nprint(conf, locs)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T06:57:26.380599Z","iopub.execute_input":"2024-06-01T06:57:26.381042Z","iopub.status.idle":"2024-06-01T06:57:26.455673Z","shell.execute_reply.started":"2024-06-01T06:57:26.381008Z","shell.execute_reply":"2024-06-01T06:57:26.454477Z"},"trusted":true},"execution_count":null,"outputs":[]}]}